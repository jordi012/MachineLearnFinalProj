## Load and Install All Packages and Libraries Needed
library(caret)
library(randomForest)
library(RColorBrewer)
library(rattle)
library(rpart)
library(rpart.plot)
library(caret)

## Create a Partition of the training Data to work with subsets of data
set.seed(1234)
training <- read.csv("C:/Users/Jordi/Desktop/pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("C:/Users/Jordi/Desktop/pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)

training2 <- training[inTrain, ]; testing2 <- training[-inTrain, ]
dim(training2); dim(testing2)
## [1] 11776   160
## [1] 7846  160

## Exclude the values with variance near to 0. To clean the model.
myDataNZV <- nearZeroVar(training2, saveMetrics=TRUE)
myNZVvars <- names(training2)

myNZVvars <- names(training2) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")

mytraining2 <- training2[!myNZVvars]

dim(training2)
##[1] 11776   100

## The first column of the dataset, looks like Primary Key or ID column. We exclude it from the model, for not being relevant.
training2 <- training2[c(-1)]

## we exclude NA values if they exceed or equal 60% of the observations.
trainingV3 <- training2 #creating another subset to iterate in loop

for(i in 1:length(training2)) { #for every column in the training dataset
        if( sum( is.na( training2[, i] ) ) /nrow(training2) >= .6 ) { #if n?? NAs > 60% of total observations
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(training2[i]), names(trainingV3)[j]) ) ==1)  { #if the columns are the same:
                trainingV3 <- trainingV3[ , -j] #Remove that column
            }   
        } 
    }
}

## Values after excluding NA's
dim(trainingV3)
## [1] 11776    58
## We save the data in our data set
training2 <- trainingV3
rm(trainingV3)

## Apply the transformations to testing dataset
clean1 <- colnames(training2)
clean2 <- colnames(training2[, -58]) #already with classe column removed
testing2 <- testing2[clean1]

dim(testing2)
## [1] 7846   58
testing <- testing[clean2]

dim(testing)
## [1] 20 57

## coerce the data type to the same one in the training and testing sets.
for (i in 1:length(testing) ) {for(j in 1:length(training2)) {
  if ( length( grep(names(training2[i]), names(testing)[j]) ) ==1)  
      {class(testing[j]) <- class(training2[i]) } } }
      
## check if the coertion really worked
testing <- rbind(training2[2, -58] , testing) 
testing <- testing[-1,]

## DECISION TREE MODEL
modFitA1 <- rpart(classe ~ ., data=training2, method="class")
fancyRpartPlot(modFitA1)

## And run the predictions as well as the Accuracy of the model onto the testing2 subdataset
predictionsA1 <- predict(modFitA1, testing2, type = "class")

## RANDOM FOREST MODEL
modFitB1 <- randomForest(classe ~. , data=training2)

## And run the predictions as well as the Accuracy of the model onto the testing2 subdataset
predictionsB1 <- predict(modFitB1, testing2, type = "class")
confusionMatrix(predictionsB1, testing2$classe)

## We finally pick up the random forest model bcs the accuracy of this model (0.998) is higher than the decision tree (0.874)
confusionMatrix(predictionsA1, testing2$classe)

## We apply the results to the testing dataset
predictionsB2 <- predict(modFitB1, testing, type = "class")
